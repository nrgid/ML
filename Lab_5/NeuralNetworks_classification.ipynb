{"cells":[{"cell_type":"markdown","metadata":{"id":"7RDY3EeAluPd"},"source":["## Learning Objectives:\n","\n","After doing this Colab, you'll know how to do the following:\n","\n","  * Create a simple deep neural network.\n","  * Tune the hyperparameters for a simple deep neural network."]},{"cell_type":"markdown","metadata":{"id":"XGj0PNaJlubZ"},"source":["## The Dataset\n","  \n","This Colab uses the California Housing Dataset"]},{"cell_type":"markdown","metadata":{"id":"xchnxAsaKKqO"},"source":["## Step 1: import relevant modules and load the dataset\n"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","\n","#mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Ev1_7TmrQXIK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X_TaJhU4KcuY"},"source":["## Load the dataset\n","\n","This exercise uses the California Housing Dataset.  The following code cell loads the separate .csv files and creates the following two pandas DataFrames:\n","\n","* `df_train`, which contains the training set\n","* `df_test`, which contains the test set\n","   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZlvdpyYKx7V"},"outputs":[],"source":["df_train =pd.read_csv(\"drive/My Drive/Colab Notebooks/Lab5_dataset_train.csv\")\n","df_test =pd.read_csv(\"drive/My Drive/Colab Notebooks/Lab5_dataset_test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8HC-TDgB1D1"},"outputs":[],"source":["# shuffle the examples\n","df_train = df_train.reindex(np.random.permutation(df_train.index)) \n","\n","\n","# Calculate the Z-scores of each column in the training set:\n","df_train_mean = df_train.mean()\n","df_train_std = df_train.std()\n","df_train_norm = (df_train - df_train_mean)/df_train_std\n","\n","# Calculate the Z-scores of each column in the test set.\n","df_test_norm = (df_test - df_train_mean)/df_train_std\n","\n","\n","df_train_norm.head()\n"]},{"cell_type":"markdown","source":["## Create binary labels"],"metadata":{"id":"PCkGPni6572P"}},{"cell_type":"code","source":["# Create Binary label\n","#75th percentile of median house value\n","print(df_train[\"median_house_value\"].quantile(q=0.75))\n","threshold = 265000.0   \n","df_train_norm[\"median_house_value_is_high\"] = (df_train[\"median_house_value\"] > threshold).astype(float)\n","df_test_norm[\"median_house_value_is_high\"] = (df_test[\"median_house_value\"] > threshold).astype(float)\n","df_train_norm[\"median_house_value_is_high\"].head()\n"],"metadata":{"id":"-0HQzgJR6AAt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b9ehCgIRjTxy"},"source":["## Represent data\n","\n","The following code cell creates a feature layer containing three features:\n","\n","* `latitude` X `longitude` (a feature cross)\n","* `median_income`\n","* `population`\n","\n","This code cell specifies the features that you'll ultimately train the model on and how each of those features will be represented. The transformations (collected in `my_feature_layer`) don't actually get applied until you pass a DataFrame to it, which will happen when we train the model. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8EkNAQhnjSu-"},"outputs":[],"source":["# Create an empty list that will eventually hold all created feature columns.\n","feature_columns = []\n","\n","# We scaled all the columns, including latitude and longitude, into their\n","# Z scores. So, instead of picking a resolution in degrees, we're going\n","# to use resolution_in_Zs.  A resolution_in_Zs of 1 corresponds to \n","# a full standard deviation. \n","resolution_in_Zs = 0.3  # 3/10 of a standard deviation.\n","\n","# Create a bucket feature column for latitude.\n","latitude_as_a_numeric_column = tf.feature_column.numeric_column(\"latitude\")\n","latitude_boundaries = list(np.arange(int(min(df_train_norm['latitude'])), \n","                                     int(max(df_train_norm['latitude'])), \n","                                     resolution_in_Zs))\n","latitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column, latitude_boundaries)\n","\n","# Create a bucket feature column for longitude.\n","longitude_as_a_numeric_column = tf.feature_column.numeric_column(\"longitude\")\n","longitude_boundaries = list(np.arange(int(min(df_train_norm['longitude'])), \n","                                      int(max(df_train_norm['longitude'])), \n","                                      resolution_in_Zs))\n","longitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column, \n","                                                longitude_boundaries)\n","\n","# Create a feature cross of latitude and longitude.\n","latitude_x_longitude = tf.feature_column.crossed_column([latitude, longitude], hash_bucket_size=100)\n","crossed_feature = tf.feature_column.indicator_column(latitude_x_longitude)\n","feature_columns.append(crossed_feature)  \n","\n","# Represent median_income as a floating-point value.\n","median_income = tf.feature_column.numeric_column(\"median_income\")\n","feature_columns.append(median_income)\n","\n","# Represent population as a floating-point value.\n","population = tf.feature_column.numeric_column(\"population\")\n","feature_columns.append(population)\n","\n","# Convert the list of feature columns into a layer that will later be fed into\n","# the model. \n","my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"]},{"cell_type":"markdown","metadata":{"id":"Ak_TMAzGOIFq"},"source":["## Step 2: build your NN\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QF0BFRXTOeR3"},"outputs":[],"source":["\n","# Define the plotting function\n","def plot_curve(epochs, hist, list_of_metrics):\n","    \"\"\"Plot a curve of one or more classification metrics vs epoch\"\"\"\n","    plt.figure()\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Value\")\n","    \n","    for m in list_of_metrics:\n","        x = hist[m]\n","        plt.plot(epochs[1:], x[1:], label=m)\n","        \n","    plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RW4Qe710LgnG"},"outputs":[],"source":["def create_model(my_learning_rate, feature_layer ,my_metrics):\n","  \"\"\"Create and compile a simple neural network model.\"\"\"\n","  # Most simple tf.keras models are sequential.\n","  model = tf.keras.models.Sequential()\n","\n","  # Add the layer containing the feature columns to the model.\n","  model.add(feature_layer)\n","\n","  # Add one linear layer \n","  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),activation=tf.sigmoid),)\n","\n","  # Construct the layers into a model that TensorFlow can execute.\n","  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n","                loss=tf.keras.losses.BinaryCrossentropy(),\n","                metrics=my_metrics)\n","\n","  return model           \n","\n","\n","def train_model(model, dataset, epochs, batch_size, label_name):\n","  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n","\n","  # Split the dataset into features and label.\n","  features = {name:np.array(value) for name, value in dataset.items()}\n","  label = np.array(features.pop(label_name))\n","  # Store your model.fit results in a 'history' variable.\n","  history = model.fit(x=features, y=label, batch_size=batch_size,\n","                      epochs=epochs, shuffle=True)\n","\n","  # Get details that will be useful for plotting the loss curve.\n","  epochs = history.epoch\n","  # Convert the history.history dictionary to a pandas dataframe.\n","  hist = pd.DataFrame(history.history)\n","\n","\n","  return epochs, hist   \n","\n","print(\"Defined the create_model and train_model functions.\")"]},{"cell_type":"markdown","metadata":{"id":"f47LmxF5X_pu"},"source":["Run the following code cell to invoke the functions defined in the preceding two code cells. (Ignore the warning messages.)\n","\n","**Note:** Depending on the version of TensorFlow, running this cell might generate WARNING messages. Please ignore these warnings. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsfE4ujDL4ju"},"outputs":[],"source":["# Hyperparameters\n","learning_rate = 0.001\n","epochs = 20\n","batch_size = 100\n","label_name = \"median_house_value_is_high\"\n","classification_threshold = 0.50\n","\n","\n","# Establish the metrics the model will measure\n","METRICS = [\n","           tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n","           tf.keras.metrics.Precision(thresholds=classification_threshold, name='precision'),\n","           tf.keras.metrics.Recall(thresholds=classification_threshold, name='recall'),\n","          ]\n","\n","my_model = create_model(learning_rate, my_feature_layer, METRICS)\n","\n","epochs, hist = train_model(my_model, df_train_norm, epochs, batch_size, label_name)\n","\n","\n","list_of_metrics_to_plot = ['accuracy','precision','recall']\n","plot_curve(epochs, hist, list_of_metrics_to_plot)\n","\n"]},{"cell_type":"markdown","source":["## Step 3: evaluate your model"],"metadata":{"id":"Qu196kh2BPW3"}},{"cell_type":"code","source":["test_features = {name:np.array(value) for name, value in df_test_norm.items()}\n","# isolate the label\n","test_label = np.array(test_features.pop(label_name)) \n","print(\"\\n Evaluate the NN model against the test set:\")\n","my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"],"metadata":{"id":"4YSz--lbBTdZ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"last_runtime":{"build_target":"//learning/deepmind/public/tools/ml_python:ml_notebook","kind":"private"},"private_outputs":true,"provenance":[{"file_id":"1fxee5LqoZcTmogHRHUyGmbR72MWxjrkw","timestamp":1669985147599},{"file_id":"https://github.com/google/eng-edu/blob/main/ml/cc/exercises/intro_to_neural_nets.ipynb","timestamp":1669974043210}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}